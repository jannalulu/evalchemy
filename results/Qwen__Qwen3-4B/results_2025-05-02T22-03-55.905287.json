{
  "results": {
    "mmlu": {
      "acc,none": 0.6823102122204814,
      "acc_stderr,none": 0.003737984989195147,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "acc,none": 0.5948990435706695,
      "acc_stderr,none": 0.006745155287183219,
      "alias": " - humanities"
    },
    "mmlu_formal_logic": {
      "alias": "  - formal_logic",
      "acc,none": 0.626984126984127,
      "acc_stderr,none": 0.04325506042017086
    },
    "mmlu_high_school_european_history": {
      "alias": "  - high_school_european_history",
      "acc,none": 0.793939393939394,
      "acc_stderr,none": 0.03158415324047711
    },
    "mmlu_high_school_us_history": {
      "alias": "  - high_school_us_history",
      "acc,none": 0.8382352941176471,
      "acc_stderr,none": 0.025845017986926917
    },
    "mmlu_high_school_world_history": {
      "alias": "  - high_school_world_history",
      "acc,none": 0.8312236286919831,
      "acc_stderr,none": 0.02438140683258622
    },
    "mmlu_international_law": {
      "alias": "  - international_law",
      "acc,none": 0.7355371900826446,
      "acc_stderr,none": 0.04026187527591205
    },
    "mmlu_jurisprudence": {
      "alias": "  - jurisprudence",
      "acc,none": 0.75,
      "acc_stderr,none": 0.04186091791394607
    },
    "mmlu_logical_fallacies": {
      "alias": "  - logical_fallacies",
      "acc,none": 0.8159509202453987,
      "acc_stderr,none": 0.03044677768797173
    },
    "mmlu_moral_disputes": {
      "alias": "  - moral_disputes",
      "acc,none": 0.6936416184971098,
      "acc_stderr,none": 0.024818350129436593
    },
    "mmlu_moral_scenarios": {
      "alias": "  - moral_scenarios",
      "acc,none": 0.37988826815642457,
      "acc_stderr,none": 0.016232826818678495
    },
    "mmlu_philosophy": {
      "alias": "  - philosophy",
      "acc,none": 0.729903536977492,
      "acc_stderr,none": 0.025218040373410616
    },
    "mmlu_prehistory": {
      "alias": "  - prehistory",
      "acc,none": 0.7469135802469136,
      "acc_stderr,none": 0.024191808600713
    },
    "mmlu_professional_law": {
      "alias": "  - professional_law",
      "acc,none": 0.4791395045632334,
      "acc_stderr,none": 0.012759117066518012
    },
    "mmlu_world_religions": {
      "alias": "  - world_religions",
      "acc,none": 0.783625730994152,
      "acc_stderr,none": 0.03158149539338733
    },
    "mmlu_other": {
      "acc,none": 0.7125844866430641,
      "acc_stderr,none": 0.007838735875892913,
      "alias": " - other"
    },
    "mmlu_business_ethics": {
      "alias": "  - business_ethics",
      "acc,none": 0.71,
      "acc_stderr,none": 0.045604802157206845
    },
    "mmlu_clinical_knowledge": {
      "alias": "  - clinical_knowledge",
      "acc,none": 0.7471698113207547,
      "acc_stderr,none": 0.026749899771241224
    },
    "mmlu_college_medicine": {
      "alias": "  - college_medicine",
      "acc,none": 0.7109826589595376,
      "acc_stderr,none": 0.034564257450869995
    },
    "mmlu_global_facts": {
      "alias": "  - global_facts",
      "acc,none": 0.32,
      "acc_stderr,none": 0.04688261722621505
    },
    "mmlu_human_aging": {
      "alias": "  - human_aging",
      "acc,none": 0.672645739910314,
      "acc_stderr,none": 0.03149384670994131
    },
    "mmlu_management": {
      "alias": "  - management",
      "acc,none": 0.8058252427184466,
      "acc_stderr,none": 0.039166677628225836
    },
    "mmlu_marketing": {
      "alias": "  - marketing",
      "acc,none": 0.8675213675213675,
      "acc_stderr,none": 0.02220930907316562
    },
    "mmlu_medical_genetics": {
      "alias": "  - medical_genetics",
      "acc,none": 0.75,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_miscellaneous": {
      "alias": "  - miscellaneous",
      "acc,none": 0.7982120051085568,
      "acc_stderr,none": 0.014351702181636854
    },
    "mmlu_nutrition": {
      "alias": "  - nutrition",
      "acc,none": 0.7320261437908496,
      "acc_stderr,none": 0.025360603796242557
    },
    "mmlu_professional_accounting": {
      "alias": "  - professional_accounting",
      "acc,none": 0.5212765957446809,
      "acc_stderr,none": 0.029800481645628693
    },
    "mmlu_professional_medicine": {
      "alias": "  - professional_medicine",
      "acc,none": 0.7279411764705882,
      "acc_stderr,none": 0.027033041151681456
    },
    "mmlu_virology": {
      "alias": "  - virology",
      "acc,none": 0.5120481927710844,
      "acc_stderr,none": 0.03891364495835816
    },
    "mmlu_social_sciences": {
      "acc,none": 0.7790055248618785,
      "acc_stderr,none": 0.007380697568715028,
      "alias": " - social sciences"
    },
    "mmlu_econometrics": {
      "alias": "  - econometrics",
      "acc,none": 0.6228070175438597,
      "acc_stderr,none": 0.04559522141958216
    },
    "mmlu_high_school_geography": {
      "alias": "  - high_school_geography",
      "acc,none": 0.8232323232323232,
      "acc_stderr,none": 0.027178752639044915
    },
    "mmlu_high_school_government_and_politics": {
      "alias": "  - high_school_government_and_politics",
      "acc,none": 0.8756476683937824,
      "acc_stderr,none": 0.02381447708659356
    },
    "mmlu_high_school_macroeconomics": {
      "alias": "  - high_school_macroeconomics",
      "acc,none": 0.7461538461538462,
      "acc_stderr,none": 0.022066054378726257
    },
    "mmlu_high_school_microeconomics": {
      "alias": "  - high_school_microeconomics",
      "acc,none": 0.8151260504201681,
      "acc_stderr,none": 0.025215992877954205
    },
    "mmlu_high_school_psychology": {
      "alias": "  - high_school_psychology",
      "acc,none": 0.8715596330275229,
      "acc_stderr,none": 0.01434497754291431
    },
    "mmlu_human_sexuality": {
      "alias": "  - human_sexuality",
      "acc,none": 0.7557251908396947,
      "acc_stderr,none": 0.037683359597287434
    },
    "mmlu_professional_psychology": {
      "alias": "  - professional_psychology",
      "acc,none": 0.7189542483660131,
      "acc_stderr,none": 0.018185218954318082
    },
    "mmlu_public_relations": {
      "alias": "  - public_relations",
      "acc,none": 0.6727272727272727,
      "acc_stderr,none": 0.0449429086625209
    },
    "mmlu_security_studies": {
      "alias": "  - security_studies",
      "acc,none": 0.7061224489795919,
      "acc_stderr,none": 0.02916273841024976
    },
    "mmlu_sociology": {
      "alias": "  - sociology",
      "acc,none": 0.8407960199004975,
      "acc_stderr,none": 0.02587064676616914
    },
    "mmlu_us_foreign_policy": {
      "alias": "  - us_foreign_policy",
      "acc,none": 0.79,
      "acc_stderr,none": 0.040936018074033256
    },
    "mmlu_stem": {
      "acc,none": 0.6885505867427847,
      "acc_stderr,none": 0.008016755476828098,
      "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
      "alias": "  - abstract_algebra",
      "acc,none": 0.59,
      "acc_stderr,none": 0.049431107042371025
    },
    "mmlu_anatomy": {
      "alias": "  - anatomy",
      "acc,none": 0.6296296296296297,
      "acc_stderr,none": 0.04171654161354543
    },
    "mmlu_astronomy": {
      "alias": "  - astronomy",
      "acc,none": 0.8026315789473685,
      "acc_stderr,none": 0.03238981601699397
    },
    "mmlu_college_biology": {
      "alias": "  - college_biology",
      "acc,none": 0.8194444444444444,
      "acc_stderr,none": 0.032166008088022675
    },
    "mmlu_college_chemistry": {
      "alias": "  - college_chemistry",
      "acc,none": 0.54,
      "acc_stderr,none": 0.05009082659620333
    },
    "mmlu_college_computer_science": {
      "alias": "  - college_computer_science",
      "acc,none": 0.66,
      "acc_stderr,none": 0.04760952285695237
    },
    "mmlu_college_mathematics": {
      "alias": "  - college_mathematics",
      "acc,none": 0.53,
      "acc_stderr,none": 0.05016135580465919
    },
    "mmlu_college_physics": {
      "alias": "  - college_physics",
      "acc,none": 0.5882352941176471,
      "acc_stderr,none": 0.04897104952726367
    },
    "mmlu_computer_security": {
      "alias": "  - computer_security",
      "acc,none": 0.79,
      "acc_stderr,none": 0.040936018074033256
    },
    "mmlu_conceptual_physics": {
      "alias": "  - conceptual_physics",
      "acc,none": 0.774468085106383,
      "acc_stderr,none": 0.027321078417387536
    },
    "mmlu_electrical_engineering": {
      "alias": "  - electrical_engineering",
      "acc,none": 0.7241379310344828,
      "acc_stderr,none": 0.037245636197746325
    },
    "mmlu_elementary_mathematics": {
      "alias": "  - elementary_mathematics",
      "acc,none": 0.6746031746031746,
      "acc_stderr,none": 0.024130158299762616
    },
    "mmlu_high_school_biology": {
      "alias": "  - high_school_biology",
      "acc,none": 0.864516129032258,
      "acc_stderr,none": 0.01946933458648693
    },
    "mmlu_high_school_chemistry": {
      "alias": "  - high_school_chemistry",
      "acc,none": 0.6995073891625616,
      "acc_stderr,none": 0.03225799476233485
    },
    "mmlu_high_school_computer_science": {
      "alias": "  - high_school_computer_science",
      "acc,none": 0.86,
      "acc_stderr,none": 0.03487350880197771
    },
    "mmlu_high_school_mathematics": {
      "alias": "  - high_school_mathematics",
      "acc,none": 0.48148148148148145,
      "acc_stderr,none": 0.030464621718895322
    },
    "mmlu_high_school_physics": {
      "alias": "  - high_school_physics",
      "acc,none": 0.5960264900662252,
      "acc_stderr,none": 0.04006485685365342
    },
    "mmlu_high_school_statistics": {
      "alias": "  - high_school_statistics",
      "acc,none": 0.6944444444444444,
      "acc_stderr,none": 0.03141554629402544
    },
    "mmlu_machine_learning": {
      "alias": "  - machine_learning",
      "acc,none": 0.5982142857142857,
      "acc_stderr,none": 0.04653333146973646
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=Qwen/Qwen3-4B,trust_remote_code=True",
    "tasks": "mmlu",
    "batch_sizes": [
      64
    ],
    "device": null,
    "use_cache": null,
    "limit": null,
    "annotator_model": "auto",
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234,
    "model_num_parameters": 4022468096,
    "model_dtype": "torch.bfloat16",
    "model_revision": "main",
    "model_sha": "82d62bb073771e7a1ea59435f548908540217d1f"
  },
  "git_hash": "8c99a65",
  "date": 1746223435.730263,
  "pretty_env_info": "'NoneType' object has no attribute 'splitlines'",
  "transformers_version": "4.51.3",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|endoftext|>",
    "151643"
  ],
  "tokenizer_eos_token": [
    "<|im_end|>",
    "151645"
  ],
  "tokenizer_bos_token": [
    null,
    "None"
  ],
  "eot_token_id": 151645,
  "max_length": 40960,
  "task_hashes": {},
  "model_source": "hf",
  "model_name": "Qwen/Qwen3-4B",
  "model_name_sanitized": "Qwen__Qwen3-4B",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 5523941.125538991,
  "end_time": 5524173.47419483,
  "total_evaluation_time_seconds": "232.34865583945066"
}